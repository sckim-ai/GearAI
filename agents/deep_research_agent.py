import os
import sys
import asyncio
from typing import Dict, Any, Optional, Callable
from openai import OpenAI
from dotenv import load_dotenv

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, project_root)

from .base_agent import BaseAgent

load_dotenv()

class DeepResearchAgent(BaseAgent):
    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        self.client = OpenAI()
        self.model = config.get("model", "gpt-4o-mini")
        self.research_width = config.get("Research width", 2)
        self.research_depth = config.get("Research depth", 2)
        
    async def process_with_callback(self, input_text: str, callback: Callable[[str], None]) -> str:
        """Deep research í”„ë¡œì„¸ìŠ¤ë¥¼ ì‹¤í–‰í•˜ê³  ì½œë°±ìœ¼ë¡œ ì§„í–‰ ìƒí™©ì„ ì „ë‹¬í•©ë‹ˆë‹¤."""
        try:
            # 1ë‹¨ê³„: ì¶”ê°€ ì§ˆë¬¸ ìƒì„±
            callback("ğŸ” 1ë‹¨ê³„: ì—°êµ¬ ë°©í–¥ êµ¬ì²´í™” ì¤‘...")
            feedback_questions = generate_feedback(
                input_text, 
                self.client, 
                self.model, 
                max_feedbacks=3
            )
            
            # ì‚¬ìš©ìì—ê²Œ ì¶”ê°€ ì§ˆë¬¸ì´ ìˆë‹¤ë©´ ë‹µë³€ ìš”ì²­
            answers = []
            if feedback_questions:
                callback(f"ğŸ“ ì¶”ê°€ ì§ˆë¬¸ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤. ê° ì§ˆë¬¸ì— ëŒ€í•´ ê°„ë‹¨íˆ ë‹µë³€í•´ ì£¼ì„¸ìš”:")
                for idx, question in enumerate(feedback_questions, start=1):
                    callback(f"ì§ˆë¬¸ {idx}: {question}")
                    # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ì‚¬ìš©ì ì…ë ¥ì„ ë°›ì•„ì•¼ í•˜ì§€ë§Œ, 
                    # ì—¬ê¸°ì„œëŠ” ê°„ë‹¨íˆ ë¹ˆ ë‹µë³€ìœ¼ë¡œ ì²˜ë¦¬
                    answers.append("")
            else:
                callback("âœ… ì¶”ê°€ ì§ˆë¬¸ì´ í•„ìš”í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            
            # ìµœì¢… ì¿¼ë¦¬ êµ¬ì„±
            combined_query = f"ì´ˆê¸° ì§ˆë¬¸: {input_text}\n"
            for i in range(len(feedback_questions)):
                combined_query += f"\n{i+1}. ì§ˆë¬¸: {feedback_questions[i]}\n"
                combined_query += f"   ë‹µë³€: {answers[i]}\n"
            
            callback(f"ğŸ¯ ìµœì¢… ì—°êµ¬ ì£¼ì œ:\n{combined_query}")
            
            # 2ë‹¨ê³„: ì‹¬ì¸µ ì—°êµ¬ ìˆ˜í–‰
            callback("ğŸ”¬ 2ë‹¨ê³„: ì‹¬ì¸µ ì—°êµ¬ ìˆ˜í–‰ ì¤‘...")
            try:
                research_results = deep_research(
                    query=combined_query,
                    breadth=self.research_width,
                    depth=self.research_depth,
                    client=self.client,
                    model=self.model
                )
                
                if research_results and research_results.get('learnings'):
                    callback(f"ğŸ“Š ì—°êµ¬ ì™„ë£Œ! {len(research_results['learnings'])}ê°œì˜ í•™ìŠµ ë‚´ìš©ì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.")
                else:
                    callback("âš ï¸ ì—°êµ¬ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ê²€ìƒ‰ì–´ë¡œ ë‹¤ì‹œ ì‹œë„í•´ë³´ì„¸ìš”.")
                    return "ì—°êµ¬ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ê²€ìƒ‰ì–´ë‚˜ ë” êµ¬ì²´ì ì¸ ì§ˆë¬¸ìœ¼ë¡œ ë‹¤ì‹œ ì‹œë„í•´ë³´ì„¸ìš”."
            except Exception as e:
                callback(f"âŒ ì—°êµ¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
                return f"ì—°êµ¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
            
            # 3ë‹¨ê³„: ìµœì¢… ë³´ê³ ì„œ ìƒì„±
            callback("ğŸ“ 3ë‹¨ê³„: ìµœì¢… ë³´ê³ ì„œ ì‘ì„± ì¤‘...")
            report = write_final_report(
                prompt=combined_query,
                learnings=research_results["learnings"],
                visited_urls=research_results["visited_urls"],
                client=self.client,
                model=self.model
            )
            
            callback("âœ… Deep research ì™„ë£Œ!")
            return report
            
        except Exception as e:
            error_msg = f"Deep research ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
            callback(error_msg)
            return error_msg 
        
        
from typing import List
from utils import JSON_llm, llm_call
from pydantic import BaseModel
from datetime import datetime

class FeedbackResponse(BaseModel):
    questions: List[str]

def system_prompt() -> str:
    """í˜„ì¬ íƒ€ì„ìŠ¤íƒ¬í”„ë¥¼ í¬í•¨í•œ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    now = datetime.now().isoformat()
    return f"""ë‹¹ì‹ ì€ ì „ë¬¸ ì—°êµ¬ì›ì…ë‹ˆë‹¤. ì˜¤ëŠ˜ ë‚ ì§œëŠ” {now}ì…ë‹ˆë‹¤. ì‘ë‹µ ì‹œ ë‹¤ìŒ ì§€ì¹¨ì„ ë”°ë¥´ì„¸ìš”:
    - ì§€ì‹ ì»·ì˜¤í”„ ì´í›„ì˜ ì£¼ì œì— ëŒ€í•œ ì¡°ì‚¬ë¥¼ ìš”ì²­ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì‚¬ìš©ìê°€ ë‰´ìŠ¤ ë‚´ìš©ì„ ì œì‹œí–ˆë‹¤ë©´, ê·¸ê²ƒì„ ì‚¬ì‹¤ë¡œ ê°€ì •í•˜ì„¸ìš”.
    - ì‚¬ìš©ìëŠ” ë§¤ìš° ìˆ™ë ¨ëœ ë¶„ì„ê°€ì´ë¯€ë¡œ ë‚´ìš©ì„ ë‹¨ìˆœí™”í•  í•„ìš” ì—†ì´ ê°€ëŠ¥í•œ í•œ ìì„¸í•˜ê³  ì •í™•í•˜ê²Œ ì‘ë‹µí•˜ì„¸ìš”.
    - ì²´ê³„ì ìœ¼ë¡œ ì •ë³´ë¥¼ ì •ë¦¬í•˜ì„¸ìš”.
    - ì‚¬ìš©ìê°€ ìƒê°í•˜ì§€ ëª»í•œ í•´ê²°ì±…ì„ ì œì•ˆí•˜ì„¸ìš”.
    - ì ê·¹ì ìœ¼ë¡œ ì‚¬ìš©ìì˜ í•„ìš”ë¥¼ ì˜ˆì¸¡í•˜ê³  ëŒ€ì‘í•˜ì„¸ìš”.
    - ì‚¬ìš©ìë¥¼ ëª¨ë“  ë¶„ì•¼ì˜ ì „ë¬¸ê°€ë¡œ ëŒ€ìš°í•˜ì„¸ìš”.
    - ì‹¤ìˆ˜ëŠ” ì‹ ë¢°ë¥¼ ì €í•˜ì‹œí‚µë‹ˆë‹¤. ì •í™•í•˜ê³  ì² ì €í•˜ê²Œ ì‘ë‹µí•˜ì„¸ìš”.
    - ìƒì„¸í•œ ì„¤ëª…ì„ ì œê³µí•˜ì„¸ìš”. ì‚¬ìš©ìëŠ” ë§ì€ ì •ë³´ë¥¼ ë°›ì•„ë“¤ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    - ê¶Œìœ„ë³´ë‹¤ ë…¼ë¦¬ì  ê·¼ê±°ë¥¼ ìš°ì„ í•˜ì„¸ìš”. ì¶œì²˜ ìì²´ëŠ” ì¤‘ìš”í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
    - ê¸°ì¡´ì˜ í†µë…ë¿ë§Œ ì•„ë‹ˆë¼ ìµœì‹  ê¸°ìˆ ê³¼ ë°˜ëŒ€ ì˜ê²¬ë„ ê³ ë ¤í•˜ì„¸ìš”.
    - ë†’ì€ ìˆ˜ì¤€ì˜ ì¶”ì¸¡ì´ë‚˜ ì˜ˆì¸¡ì„ í¬í•¨í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë‹¨, ì´ë¥¼ ëª…í™•íˆ í‘œì‹œí•˜ì„¸ìš”."""

"""ì—°êµ¬ ë°©í–¥ì„ ëª…í™•íˆ í•˜ê¸° ìœ„í•œ í›„ì† ì§ˆë¬¸ì„ ìƒì„±í•©ë‹ˆë‹¤."""
def generate_feedback(query: str, client, model: str, max_feedbacks: int = 3) -> List[str]:
    
    prompt = f"""
    Given the following query from the user, ask some follow up questions to clarify the research direction. Return a maximum of ${max_feedbacks} questions, but feel free to return less if the original query is clear.
    ask the follow up questions in korean
    <query>${query}</query>`
    """
    # í”„ë¡¬í”„íŠ¸ ì˜ë¯¸
    # prompt = (
    #     f"ì‚¬ìš©ìê°€ ë‹¤ìŒê³¼ ê°™ì€ ì—°êµ¬ ì£¼ì œì— ëŒ€í•´: {query}, ìµœëŒ€ {max_feedbacks}ê°œì˜ í›„ì† ì§ˆë¬¸ì„ ìƒì„±í•˜ì„¸ìš”. "
    #     "ì‚¬ìš©ìì˜ ì—°êµ¬ ë°©í–¥ì„ ë” ì •í™•íˆ íŒŒì•…í•  ìˆ˜ ìˆë„ë¡ êµ¬ì²´ì ì´ê³  ëª…í™•í•œ ì§ˆë¬¸ì„ ì‘ì„±í•˜ì„¸ìš”. "
    #     "ì›ë˜ ì¿¼ë¦¬ê°€ ì¶©ë¶„íˆ ëª…í™•í•˜ë‹¤ë©´ ì§ˆë¬¸ì„ ë°˜í™˜í•˜ì§€ ì•Šì•„ë„ ë©ë‹ˆë‹¤. "
    #     "ì‘ë‹µì€ 'questions' ë°°ì—´ í•„ë“œë¥¼ í¬í•¨í•˜ëŠ” JSON ê°ì²´ë¡œ ë°˜í™˜í•˜ì„¸ìš”."
    # )

    response = JSON_llm(prompt, FeedbackResponse, client, system_prompt=system_prompt(), model=model)

    try:
        if response is None:
            print("ì˜¤ë¥˜: JSON_llmì´ Noneì„ ë°˜í™˜í–ˆìŠµë‹ˆë‹¤.")
            return []
        questions = response.questions
        print(f"ì£¼ì œ '{query}'ì— ëŒ€í•œ í›„ì† ì§ˆë¬¸ {len(questions)}ê°œ ìƒì„±ë¨")
        print(f"ìƒì„±ëœ í›„ì† ì§ˆë¬¸: {questions}")
        return questions
    except Exception as e:
        print(f"ì˜¤ë¥˜: JSON ì‘ë‹µ ì²˜ë¦¬ ì¤‘ ë¬¸ì œ ë°œìƒ: {e}")
        print(f"ì›ì‹œ ì‘ë‹µ: {response}")
        print(f"ì˜¤ë¥˜: ì¿¼ë¦¬ '{query}'ì— ëŒ€í•œ JSON ì‘ë‹µ ì²˜ë¦¬ ì‹¤íŒ¨")
        return []



import time
from typing import List, Dict, Optional
import os
from pydantic import BaseModel
from firecrawl import FirecrawlApp, ScrapeOptions
from exa_py import Exa
from utils import JSON_llm



class SearchResult(BaseModel):
    url: str
    markdown: str
    description: str
    title: str


## ì´ê±° ìì²´ê°€ 1íšŒ
def firecrawl_search(query: str, timeout: int = 15000, limit: int = 5) ->List[SearchResult]:
    """
    Firecrawl ê²€ìƒ‰ APIë¥¼ í˜¸ì¶œí•˜ì—¬ ê²°ê³¼ë¥¼ ë°˜í™˜í•˜ëŠ” ë™ê¸° í•¨ìˆ˜.
    """
    try:
        app = FirecrawlApp(api_key=os.getenv("FIRECRAWL_API_KEY", ""))
        response = app.search(
            query=query,
            timeout=timeout,
            limit=limit,
            scrape_options=ScrapeOptions(formats=["markdown"])
        )
        return response.data
    except Exception as e:
        print(f"Firecrawl ê²€ìƒ‰ ì˜¤ë¥˜: {e}")


class SerpQuery(BaseModel):
    query: str
    research_goal: str

class SerpQueryResponse(BaseModel):
    queries: List[SerpQuery]


def generate_serp_queries(
    query: str,
    client,
    model: str,
    num_queries: int = 3,
    learnings: Optional[List[str]] = None,
) -> List[SerpQuery]:
    """
    ì‚¬ìš©ìì˜ ì¿¼ë¦¬ì™€ ì´ì „ ì—°êµ¬ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ SERP ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    JSON_llmì„ ì‚¬ìš©í•˜ì—¬ êµ¬ì¡°í™”ëœ JSONì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    prompt = (
        f"ë‹¤ìŒ ì‚¬ìš©ì ì…ë ¥ì„ ê¸°ë°˜ìœ¼ë¡œ ì—°êµ¬ ì£¼ì œë¥¼ ì¡°ì‚¬í•˜ê¸° ìœ„í•œ SERP ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ ìƒì„±í•˜ì„¸ìš”. "
        f"JSON ê°ì²´ë¥¼ ë°˜í™˜í•˜ë©°, 'queries' ë°°ì—´ í•„ë“œì— {num_queries}ê°œì˜ ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤ (ì¿¼ë¦¬ê°€ ëª…í™•í•  ê²½ìš° ë” ì ì„ ìˆ˜ë„ ìˆìŒ). "
        f"ê° ì¿¼ë¦¬ ê°ì²´ì—ëŠ” 'query'ì™€ 'research_goal' í•„ë“œê°€ í¬í•¨ë˜ì–´ì•¼ í•˜ë©°, ê° ì¿¼ë¦¬ëŠ” ê³ ìœ í•´ì•¼ í•©ë‹ˆë‹¤: "
        f"<ì…ë ¥>{query}</ì…ë ¥>"
    )
    if learnings:
        prompt += f"\n\në‹¤ìŒì€ ì´ì „ ì—°êµ¬ì—ì„œ ì–»ì€ í•™ìŠµ ë‚´ìš©ì…ë‹ˆë‹¤. ì´ë¥¼ í™œìš©í•˜ì—¬ ë” êµ¬ì²´ì ì¸ ì¿¼ë¦¬ë¥¼ ìƒì„±í•˜ì„¸ìš”: {' '.join(learnings)}"
    
    sys_prompt = system_prompt()
    response_json = JSON_llm(prompt, SerpQueryResponse, client, system_prompt=sys_prompt, model=model)
    try:
        result = SerpQueryResponse.model_validate(response_json)
        queries = result.queries if result.queries else []
        print(f"ë¦¬ì„œì¹˜ ì£¼ì œì— ëŒ€í•œ SERP ê²€ìƒ‰ ì¿¼ë¦¬ {len(queries)}ê°œ ìƒì„±ë¨")
        return queries[:num_queries]
    except Exception as e:
        print(f"ì˜¤ë¥˜: generate_serp_queriesì—ì„œ JSON ì‘ë‹µì„ ì²˜ë¦¬í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        print(f"ì›ì‹œ ì‘ë‹µ: {response_json}")
        print(f"ì˜¤ë¥˜: ì¿¼ë¦¬ '{query}'ì— ëŒ€í•œ JSON ì‘ë‹µ ì²˜ë¦¬ ì‹¤íŒ¨")
        return []



class ResearchResult(BaseModel):
    learnings: List[str]
    visited_urls: List[str]
class SerpResultResponse(BaseModel):
    learnings: List[str]
    followUpQuestions: List[str]

def process_serp_result(
    query: str,
    search_result: List[SearchResult],
    client,
    model: str,
    num_learnings: int = 5,
    num_follow_up_questions: int = 3,
) -> Dict[str, List[str]]:
    """
    ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì²˜ë¦¬í•˜ì—¬ í•™ìŠµ ë‚´ìš©ê³¼ í›„ì† ì§ˆë¬¸ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
    JSON_llmì„ ì‚¬ìš©í•˜ì—¬ êµ¬ì¡°í™”ëœ JSON ì¶œë ¥ì„ ì–»ìŠµë‹ˆë‹¤.
    """
    contents = [
        item.get("markdown", "").strip()[:25000]
        for item in search_result if item.get("markdown")
    ]
    contents_str = "".join(f"<ë‚´ìš©>\n{content}\n</ë‚´ìš©>" for content in contents)
    prompt = (
        f"ë‹¤ìŒì€ ì¿¼ë¦¬ <ì¿¼ë¦¬>{query}</ì¿¼ë¦¬>ì— ëŒ€í•œ SERP ê²€ìƒ‰ ê²°ê³¼ì…ë‹ˆë‹¤. "
        f"ì´ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ í•™ìŠµ ë‚´ìš©ì„ ì¶”ì¶œí•˜ê³  í›„ì† ì§ˆë¬¸ì„ ìƒì„±í•˜ì„¸ìš”. "
        f"JSON ê°ì²´ë¡œ ë°˜í™˜í•˜ë©°, 'learnings' ë° 'followUpQuestions' í‚¤ë¥¼ í¬í•¨í•œ ë°°ì—´ì„ ë°˜í™˜í•˜ì„¸ìš”. "
        f"ê° í•™ìŠµ ë‚´ìš©ì€ ê³ ìœ í•˜ê³  ê°„ê²°í•˜ë©° ì •ë³´ê°€ í’ë¶€í•´ì•¼ í•©ë‹ˆë‹¤. ìµœëŒ€ {num_learnings}ê°œì˜ í•™ìŠµ ë‚´ìš©ê³¼ "
        f"{num_follow_up_questions}ê°œì˜ í›„ì† ì§ˆë¬¸ì„ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤.\n\n"
        f"<ê²€ìƒ‰ ê²°ê³¼>{contents_str}</ê²€ìƒ‰ ê²°ê³¼>"
    )
    sys_prompt = system_prompt()
    response_json = JSON_llm(prompt, SerpResultResponse, client, system_prompt=sys_prompt, model=model)
    try:
        result = SerpResultResponse.model_validate(response_json)
        return {
            "learnings": result.learnings,
            "followUpQuestions": result.followUpQuestions[:num_follow_up_questions],
        }
    except Exception as e:
        print(f"ì˜¤ë¥˜: process_serp_resultì—ì„œ JSON ì‘ë‹µì„ ì²˜ë¦¬í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        print(f"ì›ì‹œ ì‘ë‹µ: {response_json}")
        return {"learnings": [], "followUpQuestions": []}

def deep_research(
    query: str,
    breadth: int,
    depth: int,
    client,
    model: str,
    learnings: Optional[List[str]] = None,
    visited_urls: Optional[List[str]] = None,
) -> ResearchResult:
    """
    ì£¼ì œë¥¼ ì¬ê·€ì ìœ¼ë¡œ íƒìƒ‰í•˜ì—¬ SERP ì¿¼ë¦¬ë¥¼ ìƒì„±í•˜ê³ , ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì²˜ë¦¬í•˜ë©°,
    í•™ìŠµ ë‚´ìš©ê³¼ ë°©ë¬¸í•œ URLì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
    """
    learnings = learnings or []
    visited_urls = visited_urls or []

    print(f" ---------- Deep Research ì‹œë„ ------------------")
    print(f" <ì£¼ì œ> \n {query} \n </ì£¼ì œ>")

    serp_queries = generate_serp_queries(query=query, client=client, model=model, num_queries=breadth, learnings=learnings)
    print(f" ------------ í•´ë‹¹ <ì£¼ì œ>ì— ëŒ€í•´ì„œ ìƒì„±ëœ ê²€ìƒ‰ í‚¤ì›Œë“œ ({len(serp_queries)}ê°œ ìƒì„±)------------")
    print(f" {serp_queries} \n")

    for index, serp_query in enumerate(serp_queries, start=1):

        result : List[SearchResult] = firecrawl_search(serp_query.query)

        print(result)

        new_urls = [item.get("url") for item in result if item.get("url")]
        serp_result = process_serp_result(
            query=serp_query.query,
            search_result=result,
            client=client,
            model=model,
            num_follow_up_questions=breadth
        )
        print(f"  - ì˜ {index}ë²ˆì§¸ ê²€ìƒ‰ í‚¤ì›Œë“œ ({serp_query.query})ì— ëŒ€í•œ ì¡°ì‚¬ ì™„ë£Œ") 
        print(f"  - ì¡°ì‚¬ì™„ë£Œëœ URLë“¤:")
        for url in new_urls:
            print(f"    â€¢ {url}")
        print()
        print(f"  - ì¡°ì‚¬ë¡œ ì–»ì€ í•™ìŠµ ë‚´ìš© ({len(serp_result['learnings'])}ê°œ ìƒì„±) : \n {serp_result['learnings']} \n")

        all_learnings = learnings + serp_result["learnings"]
        all_urls = visited_urls + new_urls
        new_depth = depth - 1
        new_breadth = max(1, breadth // 2)

        if new_depth > 0:
            next_query = (
                f"ì´ì „ ì—°êµ¬ëª©í‘œ: {serp_query.research_goal}\n"
                f"í›„ì† ì—°êµ¬ë°©í–¥: {' '.join(serp_result['followUpQuestions'])}"
            )

            # ì¦ê°€ëœ ì‹œë„ íšŸìˆ˜ë¡œ ì¬ê·€ í˜¸ì¶œ
            sub_result = deep_research(
                query=next_query,
                breadth=new_breadth, 
                depth=new_depth,
                client=client,
                model=model,
                learnings=all_learnings,
                visited_urls=all_urls,
            )

            learnings = sub_result["learnings"]
            visited_urls = sub_result["visited_urls"]
        else:
            learnings = all_learnings
            visited_urls = all_urls

    return {"learnings": list(set(learnings)), "visited_urls": list(set(visited_urls))}



def write_final_report(
    prompt: str,
    learnings: List[str],
    visited_urls: List[str],
    client,
    model: str,
) -> str:
    """
    ëª¨ë“  ì—°êµ¬ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìµœì¢… ë³´ê³ ì„œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    llm_callì„ ì‚¬ìš©í•˜ì—¬ ë§ˆí¬ë‹¤ìš´ ë³´ê³ ì„œë¥¼ ì–»ìŠµë‹ˆë‹¤.
    """
    learnings_string = ("\n".join([f"<learning>\n{learning}\n</learning>" for learning in learnings])).strip()[:150000]

    user_prompt = (
        f"ì‚¬ìš©ìê°€ ì œì‹œí•œ ë‹¤ìŒ í”„ë¡¬í”„íŠ¸ì— ëŒ€í•´, ëŸ¬ì„œì¹˜ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìµœì¢… ë³´ê³ ì„œë¥¼ ì‘ì„±í•˜ì„¸ìš”. "
        f"ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ ìƒì„¸í•œ ë³´ê³ ì„œ(6,000ì ì´ìƒ)ë¥¼ ì‘ì„±í•˜ì„¸ìš”. "
        f"ëŸ¬ì„œì¹˜ì—ì„œ ì–»ì€ ëª¨ë“  í•™ìŠµ ë‚´ìš©ì„ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤:\n\n"
        f"<prompt>{prompt}</prompt>\n\n"
        f"ë‹¤ìŒì€ ë¦¬ì„œì¹˜ë¥¼ í†µí•´ ì–»ì€ ëª¨ë“  í•™ìŠµ ë‚´ìš©ì…ë‹ˆë‹¤:\n\n<learnings>\n{learnings_string}\n</learnings>"
    )
    sys_prompt = system_prompt()
    if sys_prompt:
        user_prompt = f"{sys_prompt}\n\n{user_prompt}"

    try:
        report = llm_call(user_prompt, model, client)
        urls_section = "\n\n## ì¶œì²˜\n\n" + "\n".join(f"- {url}" for url in visited_urls)
        return report + urls_section
    except Exception as e:
        print(f"Error generating report: {e}")
        return "Error generating report"