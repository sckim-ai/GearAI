from openai import AsyncOpenAI, OpenAI
from typing import List
import asyncio
import json
import os
from dotenv import load_dotenv

load_dotenv()

OPENAI_API_KEY=os.getenv("OPENAI_API_KEY")

client = AsyncOpenAI(
    api_key=OPENAI_API_KEY,
)
sync_client = OpenAI(
    api_key=OPENAI_API_KEY,
)

def llm_call(promt: list[str], model: str = "gpt-4o-mini", temperature: float = 0.7) -> str:
    # messages = [promt]
    # messages.append({"role": "user", "content": promt})
    chat_completion = sync_client.chat.completions.create(
        model=model,
        temperature=temperature,
        messages=promt,
    )
    return chat_completion.choices[0].message.content

async def llm_call_async(promt: list[str], model: str = "gpt-4o-mini", temperature: float = 0.7) -> str:
    chat_completion = await client.chat.completions.create(
        model=model,
        temperature=temperature,
        messages=promt,
    )
    return chat_completion.choices[0].message.content

# 1. Prompt chaining
def prompt_chain_workflow(initial_input: list[str]) -> List[str]:
    response_chain = []
    response = initial_input[-1]["content"]

    # í”„ë¡¬í”„íŠ¸ ì²´ì¸: LLMì´ ë‹¨ê³„ì ìœ¼ë¡œ ì—¬í–‰ì„ ê³„íší•˜ë„ë¡ ìœ ë„
    prompt_chain = [
        ## ì‚¬ìš©ì ìš”ì²­ì‚¬í•­ì— ë”°ë¼ 3ê°€ì§€ ë°©ë²•ì„ ì œì‹œí•˜ê³  ê·¸ ì´ìœ ë¥¼ ì„¤ëª…
    """ì‚¬ìš©ìì˜ ìš”ì²­ì‚¬í•­ì„ ìˆ˜í–‰í•˜ê¸° ìœ„í•œ 3ê°€ì§€ ë°©ë²•ì„ ì¶”ì²œí•˜ì„¸ìš”. 
    - ë¨¼ì € ì‚¬ìš©ìê°€ ì…ë ¥í•œ ìš”ì²­ì‚¬í•­ì„ ìš”ì•½í•´ì¤˜
    - ì‚¬ìš©ìê°€ ì…ë ¥í•œ ìš”ì²­ì‚¬í•­ì„ ë°˜ì˜í•´ì„œ ì™œ ì í•©í•œ ë°©ë²•ì¸ì§€ ì„¤ëª…í•´ì£¼ì„¸ìš”
    - ê° ë°©ë²•ì˜ íŠ¹ì§•, ì¥ë‹¨ì  ë“±ì„ ì„¤ëª…í•˜ì„¸ìš”.
    """,

        ## ì¶”ì²œ ë°©ë²• ì¤‘ 1ê°€ì§€ë¥¼ ì„ íƒí•˜ê³  ë°©ë²•ì„ ìˆ˜í–‰í•˜ê¸° ìœ„í•œ í™œë™ 5ê°€ì§€ ë‚˜ì—´
    """ì£¼ì–´ì§„ ì¶”ì²œ ë°©ë²• ì¤‘ 3ê°€ì§€ ì¤‘ í•˜ë‚˜ë¥¼ ì„ íƒí•˜ì„¸ìš”. ì„ íƒí•œ ë°©ë²•ì„ ì•Œë ¤ì£¼ì„¸ìš”. ê·¸ë¦¬ê³  ì„ íƒí•œ ì´ìœ ë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”.
    - í•´ë‹¹ ë°©ë²•ì„ ìˆ˜í–‰í•˜ê¸° ìœ„í•œ ì£¼ìš” í™œë™ 5ê°€ì§€ë¥¼ ë‚˜ì—´í•˜ì„¸ìš”. 
    """,
    ]

    for i, prompt in enumerate(prompt_chain, 1):
        final_prompt = f"Prompt: {prompt}\nUser:\n{response}"
        input_promt = [{"role": "user", "content": final_prompt}]
        response = llm_call(input_promt)
        response_chain.append(response)

    return response_chain


# 2. Routing
def run_router_workflow(initial_input : list[str]):
    response_chain = []
    user_prompt = initial_input[-1]["content"]
    router_prompt = f"""
    ì‚¬ìš©ìì˜ í”„ë¡¬í”„íŠ¸/ì§ˆë¬¸: {user_prompt}

    ê° ëª¨ë¸ì€ ì„œë¡œ ë‹¤ë¥¸ ê¸°ëŠ¥ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤. ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ê°€ì¥ ì í•©í•œ ëª¨ë¸ì„ ì„ íƒí•˜ì„¸ìš”:
    - gpt-4o-mini: ê°„ë‹¨í•œ ë‹µë³€ì— ì í•©í•œ ëª¨ë¸ (ê¸°ë³¸ê°’)
    - gpt-4o: ê³ ë¯¼ì´ í•„ìš”í•œ ë‹µë³€ì— ì í•©í•œ ëª¨ë¸ 

    ëª¨ë¸ëª…ë§Œ ë‹¨ë‹µí˜•ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”
    """

    input_promt = [{"role": "user", "content": router_prompt}]
    selected_model = llm_call(input_promt)
    response_chain.append("Selected model: " + selected_model)

    response = llm_call([initial_input[-1]], model = selected_model)
    response_chain.append(response)

    return response_chain

# 3. Parallelization
async def run_Parallelization(initial_input : list[str]):
    user_prompt = initial_input[-1]["content"]

    parallel_prompt_details = [
        {"user_prompt": user_prompt, "model": "gpt-4o"},
        {"user_prompt": user_prompt, "model": "gpt-4o-mini"},
    ]

    tasks = [llm_call_async(
        promt=[{"role": "user", "content": prompt['user_prompt']}], 
        model=prompt['model']) for prompt in parallel_prompt_details]
    
    responses = []
    
    for task in asyncio.as_completed(tasks):
        result = await task
        responses.append(result)
    
    aggregator_prompt = ("ë‹¤ìŒì€ ì—¬ëŸ¬ ê°œì˜ AI ëª¨ë¸ì´ ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•´ ìƒì„±í•œ ì‘ë‹µì…ë‹ˆë‹¤.\n"
                         "ë‹¹ì‹ ì˜ ì—­í• ì€ ì´ ì‘ë‹µë“¤ì„ ëª¨ë‘ ì¢…í•©í•˜ì—¬ ìµœì¢… ë‹µë³€ì„ ì œê³µí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.\n"
                         "ì¼ë¶€ ì‘ë‹µì´ ë¶€ì •í™•í•˜ê±°ë‚˜ í¸í–¥ë  ìˆ˜ ìˆìœ¼ë¯€ë¡œ, ì‹ ë¢°ì„±ê³¼ ì •í™•ì„±ì„ ê°–ì¶˜ ì‘ë‹µì„ ìƒì„±í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤.\n\n"
                         "# ì‚¬ìš©ì ì§ˆë¬¸:\n"
                         f"{user_prompt}\n\n"
                         "# ëª¨ë¸ ì‘ë‹µë“¤:")

    for i in range(len(parallel_prompt_details)):
        responses[i] = f"{i + 1}ë²ˆ ì‘ë‹µ: {responses[i]}"
        aggregator_prompt += f"\n{responses[i]}"
    
    sum_promt = [{"role": "user", "content": aggregator_prompt}]
    final_response = await llm_call_async(sum_promt, model="gpt-4o")
    responses.append(f"ìµœì¢… ë‹µë³€: {final_response}")

    return responses

# 4. Orchestrator
async def run_llm_parallel(prompt_list : list[str]):
    tasks = [llm_call_async(promt=[{"role": "user", "content": prompt}]) for prompt in prompt_list]
    responses = []

    for task in asyncio.as_completed(tasks):
        result = await task
        responses.append(result)
    
    return responses

def get_orchestrator_prompt(user_query):
    return f"""
ë‹¤ìŒ ì‚¬ìš©ì ì§ˆë¬¸ì„ ë¶„ì„í•˜ê³ , ì´ë¥¼ ê´€ë ¨ëœ í•˜ìœ„ ì§ˆë¬¸ìœ¼ë¡œ ë¶„í•´í•˜ì‹­ì‹œì˜¤:

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì‘ë‹µì„ ì œê³µí•˜ì‹­ì‹œì˜¤:

{{
    "analysis": "ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•œ ì´í•´ë¥¼ ìƒì„¸íˆ ì„¤ëª…í•˜ê³ , ì‘ì„±í•œ í•˜ìœ„ ì§ˆë¬¸ë“¤ì˜ ê·¼ê±°ë¥¼ ì„¤ëª…í•˜ì‹­ì‹œì˜¤.",
    "subtasks": [
        {{
            "description": "ì´ í•˜ìœ„ ì§ˆë¬¸ì˜ ì´ˆì ê³¼ ì˜ë„ë¥¼ ì„¤ëª…í•˜ì‹­ì‹œì˜¤.",
            "sub_question": "ì§ˆë¬¸ 1"
        }},
        {{
            "description": "ì´ í•˜ìœ„ ì§ˆë¬¸ì˜ ì´ˆì ê³¼ ì˜ë„ë¥¼ ì„¤ëª…í•˜ì‹­ì‹œì˜¤.",
            "sub_question": "ì§ˆë¬¸ 2"
        }}
        // í•„ìš”ì— ë”°ë¼ ì¶”ê°€ í•˜ìœ„ ì§ˆë¬¸ í¬í•¨
    ]
}}
ì§ˆë¬¸ì˜ ìˆ˜ì¤€ì— ë”°ë¼ ìµœëŒ€ 5ê°œì˜ í•˜ìœ„ ì§ˆë¬¸ì„ ìƒì„±í•˜ì„¸ìš”.

# ì‚¬ìš©ì ì§ˆë¬¸: {user_query}
"""

def get_worker_prompt(user_query, sub_question, description):
    return f"""
    ë‹¤ìŒ ì‚¬ìš©ì ì§ˆë¬¸ì—ì„œ íŒŒìƒëœ í•˜ìœ„ ì§ˆë¬¸ì„ ë‹¤ë£¨ëŠ” ì‘ì—…ì„ ë§¡ì•˜ìŠµë‹ˆë‹¤:
    \nì›ë˜ ì§ˆë¬¸:  {user_query}
    \ní•˜ìœ„ ì§ˆë¬¸: {sub_question}

    ì§€ì¹¨: {description}

    í•˜ìœ„ ì§ˆë¬¸ì„ ì² ì €íˆ ë‹¤ë£¨ëŠ” í¬ê´„ì ì´ê³  ìƒì„¸í•œ ì‘ë‹µì„ í•´ì£¼ì„¸ìš”
    """

async def orchestrate_task(initial_input : list[str]):
    user_query = initial_input[-1]["content"]    
    # 1ë‹¨ê³„ : ì‚¬ìš©ì ì§ˆë¬¸ ê¸°ë°˜ìœ¼ë¡œ ì—¬ëŸ¬ ì§ˆë¬¸ ë„ì¶œ
    orchestrator_prompt = get_orchestrator_prompt(user_query)
    input_promt = [{"role": "user", "content": orchestrator_prompt}]
    orchestrator_response = llm_call(input_promt, model="gpt-4o")
     
    response_json = json.loads(orchestrator_response.replace('```json', '').replace('```', ''))   
    analysis = response_json.get("analysis", "")
    sub_tasks = response_json.get("subtasks", [])

    # 2ë‹¨ê³„ : ê° í•˜ìœ„ì§ˆë¬¸ì— ëŒ€í•œ LLM í˜¸ì¶œ
    worker_prompts = [get_worker_prompt(user_query, task["sub_question"], task["description"]) for task in sub_tasks]     
    worker_responses = await run_llm_parallel(worker_prompts)
        
    # 3ë‹¨ê³„ : í•˜ìœ„ì§ˆë¬¸ ì‘ë‹µ ì¢…í•© ë° LLM í˜¸ì¶œ
    aggregator_prompt = f"""ì•„ë˜ëŠ” ì‚¬ìš©ìì˜ ì›ë˜ ì§ˆë¬¸ì— ëŒ€í•´ì„œ í•˜ìœ„ ì§ˆë¬¸ì„ ë‚˜ëˆ„ê³  ì‘ë‹µí•œ ê²°ê³¼ì…ë‹ˆë‹¤.
    ì•„ë˜ ì§ˆë¬¸ ë° ì‘ë‹µë‚´ìš©ì„ í¬í•¨í•œ ìµœì¢… ì‘ë‹µì„ ì œê³µí•´ì£¼ì„¸ìš”.
    ## ìš”ì²­ì‚¬í•­
    - í•˜ìœ„ì§ˆë¬¸ ì‘ë‹µë‚´ìš©ì´ ìµœëŒ€í•œ í¬ê´„ì ì´ê³  ìƒì„¸í•˜ê²Œ í¬í•¨ë˜ì–´ì•¼ í•©ë‹ˆë‹¤
    ì‚¬ìš©ìì˜ ì›ë˜ ì§ˆë¬¸:
    {user_query}

    í•˜ìœ„ ì§ˆë¬¸ ë° ì‘ë‹µ:
    """
    
    for i in range(len(sub_tasks)):
        aggregator_prompt += f"\n{i+1}. í•˜ìœ„ ì§ˆë¬¸: {sub_tasks[i]['sub_question']}\n"
        aggregator_prompt += f"\n   ì‘ë‹µ: {worker_responses[i]}\n"
        
    final_response = llm_call(promt=[{"role": "user", "content": aggregator_prompt}], model="gpt-4o")
    
    return final_response

# 5. Evaluator optimizer
def loop_workflow(initial_input, evaluator_prompt, max_retries=5) -> str:
    """í‰ê°€ìê°€ ìƒì„±ëœ ìš”ì•½ì„ í†µê³¼í•  ë•Œê¹Œì§€ ìµœëŒ€ max_retriesë²ˆ ë°˜ë³µ."""
    user_query = initial_input[-1]["content"]    

    retries = 0
    while retries < max_retries:
        print(f"\n========== ğŸ“ ìš”ì•½ í”„ë¡¬í”„íŠ¸ (ì‹œë„ {retries + 1}/{max_retries}) ==========\n")
        print(user_query)
        
        summary = llm_call(user_query, model="gpt-4o-mini")
        print(f"\n========== ğŸ“ ìš”ì•½ ê²°ê³¼ (ì‹œë„ {retries + 1}/{max_retries}) ==========\n")
        print(summary)
        
        final_evaluator_prompt = evaluator_prompt + summary
        evaluation_result = llm_call(final_evaluator_prompt, model="gpt-4o").strip()

        print(f"\n========== ğŸ” í‰ê°€ í”„ë¡¬í”„íŠ¸ (ì‹œë„ {retries + 1}/{max_retries}) ==========\n")
        print(final_evaluator_prompt)

        print(f"\n========== ğŸ” í‰ê°€ ê²°ê³¼ (ì‹œë„ {retries + 1}/{max_retries}) ==========\n")
        print(evaluation_result)

        if "í‰ê°€ê²°ê³¼ = PASS" in evaluation_result:
            print("\nâœ… í†µê³¼! ìµœì¢… ìš”ì•½ì´ ìŠ¹ì¸ë˜ì—ˆìŠµë‹ˆë‹¤.\n")
            return summary
        
        retries += 1
        print(f"\nğŸ”„ ì¬ì‹œë„ í•„ìš”... ({retries}/{max_retries})\n")

        # If max retries reached, return last attempt
        if retries >= max_retries:
            print("âŒ ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ë„ë‹¬. ë§ˆì§€ë§‰ ìš”ì•½ì„ ë°˜í™˜í•©ë‹ˆë‹¤.")
            return summary  # Returning the last attempted summary, even if it's not perfect.

        # Updating the user_query for the next attempt with full history
        user_query += f"{retries}ì°¨ ìš”ì•½ ê²°ê³¼:\n\n{summary}\n"
        user_query += f"{retries}ì°¨ ìš”ì•½ í”¼ë“œë°±:\n\n{evaluation_result}\n\n"


if __name__ == "__main__":
    test = llm_call([{"role": "user", "content": "ì•ˆë…•"}])
    print(test)